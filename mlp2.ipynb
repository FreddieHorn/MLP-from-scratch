{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, transfer_functions):\n",
    "        # Check the number of layers\n",
    "        assert len(layer_sizes) - 1 == len(transfer_functions), \"Number of hidden layers and transfer functions must be the same\"\n",
    "        assert len(layer_sizes) > 1 and len(layer_sizes) <= 4, \"Number of layers should be between 2 and 4\"\n",
    "\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # Initialize network structure\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.transfer_functions = transfer_functions\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = [np.random.uniform(-2.0, 2.0, (layer_sizes[i+1], layer_sizes[i])) for i in range(len(layer_sizes)-1)]\n",
    "        self.biases = [np.random.uniform(-2.0, 2.0, (layer_sizes[i+1], 1)) for i in range(len(layer_sizes)-1)]\n",
    "\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def __tanh_derivative(self, x):\n",
    "        return 1 - x**2\n",
    "    \n",
    "    def get_activation_function(self, input, function_name):\n",
    "        match function_name:\n",
    "            case 'logistic':\n",
    "                return self.__sigmoid(input)\n",
    "            case 'tanh': \n",
    "                return np.tanh(input)\n",
    "            case 'identity': \n",
    "                return input\n",
    "            \n",
    "    def get_activation_function_gradient(self, input, function_name):\n",
    "        match function_name:\n",
    "            case 'logistic':\n",
    "                return self.__sigmoid_derivative(input)\n",
    "            case 'tanh': \n",
    "                return 1 - self.__tanh_derivative(input)\n",
    "            case 'identity': \n",
    "                return 1\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.activations = [inputs]\n",
    "        self.weighted_sums = []\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            weighted_sum = np.dot(self.weights[i], self.activations[i]) + self.biases[i]\n",
    "            activation = self.get_activation_function(weighted_sum, self.transfer_functions[i])\n",
    "            x = self.transfer_functions[i]\n",
    "            self.activations.append(activation)\n",
    "\n",
    "        return self.activations[-1]\n",
    "\n",
    "    def backward(self, inputs, targets, learning_rates):\n",
    "        # Backward pass\n",
    "        x = self.transfer_functions[-1]\n",
    "        errors = [targets - self.activations[-1]]\n",
    "        deltas = [errors[-1] * self.get_activation_function_gradient(self.activations[-1], self.transfer_functions[-1])]\n",
    "\n",
    "        for i in reversed(range(len(self.layer_sizes)-2)):\n",
    "            error = np.dot(self.weights[i+1].T, deltas[0])\n",
    "            errors.insert(0, error)\n",
    "            delta = errors[0] * self.get_activation_function_gradient(self.activations[i+1], self.transfer_functions[i])\n",
    "            deltas.insert(0, delta)\n",
    "\n",
    "        # Update weights and biases\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            self.weights[i] += learning_rates[i] * np.dot(deltas[i], self.activations[i].T)\n",
    "            self.biases[i] += learning_rates[i] * deltas[i]\n",
    "\n",
    "    def mean_squared_error(self, targets):\n",
    "        return np.mean((targets - self.activations[-1])**2)\n",
    "\n",
    "    def train(self, inputs, targets, epochs, learning_rates):\n",
    "        learning_curve = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_error = 0\n",
    "\n",
    "            for i in range(len(inputs)):\n",
    "                input_data = np.array(inputs[i]).reshape(-1, 1)\n",
    "                target_data = np.array(targets[i]).reshape(-1, 1)\n",
    "\n",
    "                # Forward pass\n",
    "                self.forward(input_data)\n",
    "\n",
    "                # Calculate error\n",
    "                error = self.mean_squared_error(target_data)\n",
    "                total_error += error\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(input_data, target_data, learning_rates)\n",
    "\n",
    "            # Calculate and store mean error for the epoch\n",
    "            mean_error = total_error / len(inputs)\n",
    "            learning_curve.append(mean_error)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Mean Squared Error: {mean_error}\")\n",
    "\n",
    "        # Save learning curve to a file\n",
    "        with open(\"learning_curve.txt\", \"w\") as lc_file:\n",
    "            lc_file.write(\"\\n\".join(map(str, learning_curve)))\n",
    "\n",
    "    def test(self, test_inputs, test_targets):\n",
    "        predictions = self.predict(test_inputs)\n",
    "        test_error = np.mean((test_targets - predictions)**2)\n",
    "\n",
    "        print(f\"\\nTest Mean Squared Error: {test_error}\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        predictions = []\n",
    "        for i in range(len(inputs)):\n",
    "            input_data = np.array(inputs[i]).reshape(-1, 1)\n",
    "            output = self.forward(input_data)\n",
    "            predictions.append(output.flatten())\n",
    "        return predictions\n",
    "    \n",
    "def load_data(path):\n",
    "    with open(path, 'r') as file:\n",
    "        file.readline()\n",
    "\n",
    "        parameters = file.readline().split()\n",
    "\n",
    "        P, N, M = map(lambda x: int(x.split('=')[1]), parameters[1:])\n",
    "\n",
    "        data = np.loadtxt(file, dtype=np.dtype(np.float64))\n",
    "    #data = np.subtract(data, 0.1) #for some reason, 0.1 is added to all values in the file?\n",
    "    # Extract input and output matrices\n",
    "    input_matrix = data[:, :N]\n",
    "    output_matrix = data[:, N:]\n",
    "\n",
    "    return input_matrix, output_matrix, P, N, M\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Set network parameters\n",
    "    path = \"PA-A_training_data_03.txt\"\n",
    "    test_path = \"test_data.txt\"\n",
    "    X_test, Y_test, _, _, _ = load_data(path=test_path)\n",
    "    X_train, Y_train, P, N, M = load_data(path=path)\n",
    "\n",
    "    layer_sizes = [N, 100, 20, M]  # Number of neurons in each layer\n",
    "    transfer_functions = ['logistic', 'logistic', 'tanh']  # Transfer function for each layer\n",
    "    learning_rates = [0.01, 0.005, 0.001]\n",
    "    # Create MLP\n",
    "    mlp = MLP(layer_sizes, transfer_functions)\n",
    "\n",
    "    # Train the MLP\n",
    "    mlp.train(X_train, Y_train, epochs=100000, learning_rates = learning_rates)\n",
    "\n",
    "    # Test the trained network\n",
    "    mlp.test(X_test, Y_test)\n",
    "\n",
    "    # Plot learning curve\n",
    "    learning_curve = np.loadtxt(\"learning_curve.txt\")\n",
    "    plt.plot(learning_curve)\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
